{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4931bccb-5a74-4c75-830a-8eec54c03166",
   "metadata": {},
   "source": [
    "# minGemma\n",
    "\n",
    "our goal here is to teach all of the architectural concepts used in [Google's Gemma](https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf). The code here is a frankensteinian mixture between a little bit of [Andrej Karpathy's minGPT](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=5012s) and mostly [Google's pytorch implementation of Gemma](https://github.com/google/gemma_pytorch). As such, certain aspects of the Gemma model will or won't be included based on our learning goals\n",
    "\n",
    "#### Included:\n",
    "- RoPE\n",
    "- RMSnorm\n",
    "- GeGLU activation\n",
    "- Multi-Query Attention (for the smaller model)\n",
    "- The extra normalization in the middle of each layer, between the attention mechanism and the MLP\n",
    "\n",
    "#### Not Included: \n",
    "- **Gemma's training dataset:** The dataset & corresponding RLHF framework used to train Gemmma is out-of-scope for this lesson. This is unfortunate since a big part of Gemma's impressive performance is attributed to their high quantity and quality of data. I recommend reading the technical report for more information\n",
    "- **Tokenization training:** The full 256,128 token vocabulary of Gemma is unusually large even for models orders of magnitude larger than Gemma (if I remember correctly, Llama's is around 32k). Their tokenizer also has interesting qualities such as beginning-of-sentence & end-of-sentence tokens & other tokens related to instruct tuning. The issue with training such a large token vocabulary is that many tokens are likely to rarely/never be trained, thus meaning you've wasted compute and leaving yourself vulnerable to the [SolidGoldMagikarp problem](https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation) unless you use an absurdly large dataset size like they did (2T and 6T tokens for the 2b and 7b models respectively). That being said, a better lesson than mine would've taught you how to train a more reasonably sized vocabulary. Instead of doing that, I recommend for homework you check out [Andrej Karpathy's recent guide on building tokenizers](https://www.youtube.com/watch?v=zduSFxRajkE) and implement a reasonably sized one yourself. Instead of that, we'll be using simple character-wise tokenization of TinyShakespeare by default, but the code will include the option to use Gemma's original gigantic tokenizer just so you can see what it looks like.\n",
    "- **Highly parallelized training:** This is something I don't believe google included in their open-sourcing of these models which is unfortunate, but reasonable given that they trained on TPUs and no one owns TPUs except google. Rather, their open-sourced code (at least the pytorch version) is designed strictly for inference with their open-sourced weights. I recommend looking elsewhere if you want to learn about parallelized training techniques like sharding on GPUs\n",
    "- **KV caching:** A standard method for saving on compute in transformers, KV caching involves saving individual key and value vectors calculated at each step of inference for use at future steps of inference. I've not included it here because it's an inference-specific mechanism whereas this code is meant to train, but if you develop your own inference version of this code then you 100% need to re-implment KV caching\n",
    "- **Quantization:** While you could certainly choose to train at different floating point precision from the default, that will be specific to your own compute requirements and shouldn't be necessary for the absolutely tiny size of the default parameters I've set for minGemma\n",
    "\n",
    "### ToDo\n",
    "- look in-depth through the RoPE comments. I let ChatGPT write all of that so idk how much sense they make\n",
    "- fill in all the empty links i've setup\n",
    "- look into & talk about GeGLU in the MLP\n",
    "- see if i can remove typing from the attention softmax calculation\n",
    "- see if i can push an update to the official google repo fixing GammaDecoderLayer()'s original call of `config: gemma_config.GemmaConfig` to `config: GemmaConfig`. Not sure how i'd test it on their version to make sure it works before messing with it tho. might not make any sense given the import statement they use\n",
    "- figure out this tokenizer problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "562f334c-b16b-42ab-830c-03761b4daf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# config.py\n",
    "import dataclasses # used for the config. this library is good at making classes that are full of just datatypes\n",
    "from typing import Optional\n",
    "\n",
    "# model.py\n",
    "import re\n",
    "from typing import Any, List, Sequence, Tuple, Union#, Optional\n",
    "#from gemma import config as gemma_config\n",
    "#from gemma import tokenizer\n",
    "\n",
    "# optionally downloading the original tokenizer\n",
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d031a96f-6264-43c8-8647-ac8f131c2ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "#device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cf7fba-5e7e-49a9-80e9-1064007a68b4",
   "metadata": {},
   "source": [
    "# The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bd373bf-d326-4939-be18-819bae892805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n",
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] 65\n"
     ]
    }
   ],
   "source": [
    "# the dataset we'll be using is just TinyShakespeare\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(text[:200])\n",
    "\n",
    "# here are all the unique characters that occur in this text. we'll do character-wise tokenization\n",
    "chars = sorted(list(set(text)))\n",
    "v = len(chars)\n",
    "print(chars, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff6dd918-45f8-41d7-a229-d74cfe84c59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb3086ce-a3bf-4a8a-ad3d-ff1730ddb051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning it into an actual tokenizer\n",
    "# this version was created by ChatGPT. not super confident in it. gonna watch Andrej's video on making a custom tokenizer\n",
    "# also need to change the rest of the references to use this tokenizer rather\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, chars: List[str]):\n",
    "        # Create mappings from characters to integers and vice versa\n",
    "        self.stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "        self.itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "        ### not sure i need all these. I think they make it compatible with later code\n",
    "        # Special tokens, adjust as needed\n",
    "        self.bos_char = '<'  # Beginning of string character\n",
    "        self.eos_char = '>'  # End of string character\n",
    "        self.pad_char = '~'  # Padding character\n",
    "\n",
    "        # Add special tokens to mappings\n",
    "        special_tokens = [self.bos_char, self.eos_char, self.pad_char]\n",
    "        for token in special_tokens:\n",
    "            if token not in self.stoi:\n",
    "                index = len(self.stoi)\n",
    "                self.stoi[token] = index\n",
    "                self.itos[index] = token\n",
    "\n",
    "        # Update vocabulary size\n",
    "        self.n_words = len(self.stoi)\n",
    "\n",
    "        # Define special token IDs\n",
    "        self.bos_id = self.stoi[self.bos_char]\n",
    "        self.eos_id = self.stoi[self.eos_char]\n",
    "        self.pad_id = self.stoi[self.pad_char]\n",
    "\n",
    "    def encode(self, s: str, bos: bool = True, eos: bool = True) -> List[int]:\n",
    "        \"\"\"Converts a string into a list of character IDs, optionally adding BOS/EOS tokens.\"\"\"\n",
    "        if bos:\n",
    "            s = self.bos_char + s\n",
    "        if eos:\n",
    "            s += self.eos_char\n",
    "        return [self.stoi.get(c, self.stoi[self.pad_char]) for c in s]\n",
    "\n",
    "    def decode(self, t: List[int]) -> str:\n",
    "        \"\"\"Converts a list of character IDs back into a string.\"\"\"\n",
    "        return ''.join([self.itos.get(i, self.pad_char) for i in t])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb41f20-88c2-4371-a064-f748977d8d85",
   "metadata": {},
   "source": [
    "### these are all screwy now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcc4a903-1131-4747-9a9e-060c9eef299c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest validation\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33d02be1-d005-4913-b8b7-630a7b97696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading for training\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - t, (b,))\n",
    "    x = torch.stack([data[i:i+t] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+t+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba904bf4-cfbd-4a30-aafe-d76971283b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(): # to estimate loss later during the training loop\n",
    "    out = {}\n",
    "    model.eval() # sets model to eval mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # just resets to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95ec592-187f-4215-b88a-ec894f35e4e4",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4840a486-e57b-4c71-b8a5-4db59b63b068",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class GemmaConfig:\n",
    "    # The number of tokens in the vocabulary.\n",
    "    vocab_size: int = v # defined earlier when we loaded TinyShakespeare. In Gemma it's 256,128\n",
    "    \n",
    "    # The maximum sequence length that this model might ever be used with.\n",
    "    max_position_embeddings: int = 64 # In Gemma it's 8192\n",
    "    \n",
    "    # The number of blocks in the model.\n",
    "    num_hidden_layers: int = 4 # In Gemma 7b it's 28 and 2b it's 18\n",
    "    \n",
    "    # The number of attention heads used in the attention layers of the model.\n",
    "    num_attention_heads: int = 4 # In Gemma 7b it's 16 and 2b it's 8\n",
    "    \n",
    "    # The number of key-value heads for implementing attention.\n",
    "    num_key_value_heads: int = 1 # In Gemma 7b it's 16 and in 2b it's 1\n",
    "    # Notice what with Gemma 7b num_attention_heads = num_key_value_heads whereas this is not true for 2b\n",
    "    # this is because 7b uses regular multi-head attention while 2b uses multi-query attention\n",
    "    # the difference is that MQA shares single key & value matrices across all heads whereas MHA\n",
    "    \n",
    "    # The hidden size of the model.\n",
    "    hidden_size: int = 128 # In Gemma 7b it's 3072 and in 2b it's 2048\n",
    "    \n",
    "    # The dimension of the MLP representations.\n",
    "    intermediate_size: int = 1024 # In Gemma 7b it's 24576 and in 2b it's 16384\n",
    "    \n",
    "    # The number of head dimensions.\n",
    "    head_dim: int = 32 # In both Gemmas it's 256\n",
    "    \n",
    "    # The epsilon used by the rms normalization layers.\n",
    "    rms_norm_eps: float = 1e-6\n",
    "    \n",
    "    # The path to the model tokenizer.\n",
    "    tokenizer: Optional[str] = 'tokenizer/characterWiseTokenizer.model'\n",
    "    # instead of the original, we'll be using the functions defined above for our character-wise tokenization\n",
    "    \n",
    "    # The dtype of the weights.\n",
    "    #dtype: str = 'bfloat16'\n",
    "    \n",
    "    # Whether a quantized version of the model is used.\n",
    "    # quant: bool = False \n",
    "    # we won't be providing this option in our implementation. All corresponding code for it has been removed\n",
    "\n",
    "    #def get_dtype(self) -> Optional[torch.dtype]:\n",
    "        #\"\"\"Gets the torch dtype from the config dtype string.\"\"\"\n",
    "        #return _STR_DTYPE_TO_TORCH_DTYPE.get(self.dtype, None)\n",
    "\n",
    "\n",
    "def get_config_for_7b() -> GemmaConfig:\n",
    "    return GemmaConfig(\n",
    "        vocab_size = 256128,\n",
    "        max_position_embeddings = 8192,\n",
    "        num_hidden_layers = 28,\n",
    "        num_attention_heads = 16,\n",
    "        num_key_value_heads = 16,\n",
    "        hidden_size = 3072,\n",
    "        intermediate_size = 24576,\n",
    "        head_dim = 256,\n",
    "        tokenizer = 'tokenizer/tokenizer.model'\n",
    "    )\n",
    "\n",
    "\n",
    "def get_config_for_2b() -> GemmaConfig:\n",
    "    return GemmaConfig(\n",
    "        vocab_size = 256128,\n",
    "        max_position_embeddings = 8192,\n",
    "        num_hidden_layers = 18,\n",
    "        num_attention_heads = 8,\n",
    "        num_key_value_heads = 1,\n",
    "        hidden_size = 2048,\n",
    "        intermediate_size = 16384,\n",
    "        head_dim = 256,\n",
    "        tokenizer = 'tokenizer/tokenizer.model'\n",
    "    )\n",
    "\n",
    "def download_original_tokenizer():\n",
    "    \"\"\"\n",
    "    I'm too lazy to read through google's license and figure out whether it's ok for me to download & redistribute their tokenizer,\n",
    "    so instead i just wrote a script that has you download it yourself\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    DESTINATION_FOLDER_PATH = './tokenizer'  # Local path in your Jupyter environment\n",
    "    \n",
    "    # Construct the URL to the raw file on GitHub\n",
    "    url = 'https://raw.githubusercontent.com/google/gemma_pytorch/main/tokenizer/tokenizer.model'\n",
    "    \n",
    "    # Make the HTTP GET request\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Ensure the destination folder exists\n",
    "        os.makedirs(DESTINATION_FOLDER_PATH, exist_ok=True)\n",
    "        \n",
    "        # Define the local path to save the file\n",
    "        local_file_path = os.path.join(DESTINATION_FOLDER_PATH, os.path.basename('tokenizer.model'))\n",
    "        \n",
    "        # Write the content of the response to a local file\n",
    "        with open(local_file_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f'File successfully downloaded to {local_file_path}')\n",
    "    else:\n",
    "        print(f'Failed to download the file. HTTP status code: {response.status_code}')\n",
    "\n",
    "def get_model_config(variant: str) -> GemmaConfig:\n",
    "    if variant == '7b':\n",
    "        return get_config_for_7b()\n",
    "    elif variant == '2b':\n",
    "        return get_config_for_2b()\n",
    "    #return ValueError(f'Invalid variant {variant}. Supported variants are \"2b\" and \"7b\"')\n",
    "    else:\n",
    "        return GemmaConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb91f1f-a514-4769-8ad7-067ff39ef0e0",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f49711ca-8da5-46a7-b877-5c3a5168ba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler(nn.Module):\n",
    "    \"\"\"\n",
    "    The Sampler class is responsible for generating token predictions from Gemma's output.\n",
    "    It supports temperature scaling, top-p (nucleus) sampling, and top-k sampling to control the diversity and quality of the generated text. \n",
    "    The class operates as follows:\n",
    "\n",
    "    1. Selects the last hidden state for each sequence in the batch\n",
    "\n",
    "    2. Computes logits by multiplying the selected hidden states with the transposed embedding matrix. \n",
    "    An optional embedding bias can be added to these logits to bias the predictions towards certain tokens.\n",
    "\n",
    "    3. If no temperature is provided, the function returns the indices of the tokens with the highest logits, aka greedy decoding.\n",
    "\n",
    "    4. If a temperature is provided, it is used to scale the logits, making the distribution over tokens sharper (lower temperature) \n",
    "    or flatter (higher temperature), which affects the randomness of the sampling.\n",
    "\n",
    "    5. The softmax function is applied to the scaled logits to obtain a probability distribution over the vocabulary.\n",
    "\n",
    "    6. For top-p sampling, the function computes the cumulative sum of the sorted probabilities and masks out tokens until the \n",
    "    cumulative probability exceeds the threshold defined by `top_ps`. This allows the model to focus on a subset of the most \n",
    "    probable tokens while ignoring the long tail of less likely tokens.\n",
    "\n",
    "    7. For top-k sampling, the function masks out all tokens except the `k` most likely ones, as specified by `top_ks`. \n",
    "    This ensures that the model only considers a fixed number of the most probable tokens for the next token prediction.\n",
    "\n",
    "    8. After applying both the top-p and top-k masks, the probabilities are re-normalized so that they sum up to 1\n",
    "\n",
    "    9. The function then samples from the re-normalized probability distribution to select the next token. \n",
    "    This step introduces randomness into the text generation process, allowing the model to generate diverse and coherent text.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(\n",
    "        self,\n",
    "        embedding: torch.Tensor, # shape (vocab_size, hidden_size)\n",
    "        hidden_states: torch.Tensor, # shape (batch_size, input_len, hidden_size)\n",
    "        output_positions: torch.Tensor, # shape (batch_size) list of integer indices. I think they should be all -1's for NTP\n",
    "        temperatures: torch.Tensor, # shape (batch_size) list of float temperatures\n",
    "        top_ps: torch.Tensor, # shape (batch_size) list of float percentages to denote out top_p criteria\n",
    "        top_ks: torch.Tensor, # shape (batch_size) list of integers to denote our top_k criteria\n",
    "        embedding_bias: Optional[torch.Tensor] = None, # shape (vocab_size) meant to bias output towards certain tokens\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        # Select the last element for each sequence.\n",
    "        # (batch_size, input_len, hidden_size) -> (batch_size, hidden_size)\n",
    "        # the \"1\" selects along the 'input_len' dimension & `output_positions` is a list of indices\n",
    "        hidden_states = hidden_states.index_select(1, output_positions).squeeze(dim=1) # squeeze removes the input_len dimension\n",
    "        \n",
    "        # the embedding matrix is also used as the output matrix. this saves on parameters\n",
    "        # (batch_size, hidden_size) @ (hidden_size, vocab_size) -> (batch_size, vocab_size)\n",
    "        logits = torch.matmul(hidden_states, embedding.t())\n",
    "        if embedding_bias is not None:\n",
    "            # (batch_size, vocab_size) + (vocab_size) -> (batch_size, vocab_size)\n",
    "            logits += embedding_bias\n",
    "\n",
    "        if temperatures is None:\n",
    "            # selects the token with the largest logit. No need to do softmax since there's no temperature\n",
    "            return torch.argmax(logits, dim=-1).squeeze(dim=-1)\n",
    "\n",
    "        # Apply temperature scaling\n",
    "        # (batch_size, vocab_size) / (batch_size) -> (batch_size, vocab_size)\n",
    "        logits.div_(temperatures.unsqueeze(dim=1))\n",
    "\n",
    "        # Calculate probabilities with softmax.\n",
    "        probs = torch.softmax(logits, dim=-1, dtype=torch.float) # dim=-1 is the vocab_size dimension\n",
    "\n",
    "        # sort the probabilities to for use in top-p & top-k\n",
    "        probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True) # both (batch_size, vocab_size)\n",
    "\n",
    "        # calculating top-p\n",
    "        # creates same-size tensor of cumulatve probabilities instead of indivdiual probs\n",
    "        probs_sum = torch.cumsum(probs_sort, dim=-1) \n",
    "        # mask where 0's are top-p selections & 1's are to be excluded\n",
    "        top_ps_mask = (probs_sum - probs_sort) > top_ps.unsqueeze(dim=1) \n",
    "        # the original probabilities with excluded tokens changed to 0.0\n",
    "        probs_sort = torch.where(top_ps_mask, 0, probs_sort) \n",
    "\n",
    "        # calculating top_k\n",
    "        # create a shape (vocab_size) tensor that just iterates up by 1's\n",
    "        top_ks_mask = torch.arange(probs_idx.shape[-1], device=probs_idx.device) \n",
    "        # expand our mask along the batch_size dimension to become size (batch_size, vocab_size)\n",
    "        # \"expand\" means copy the original into this new size, so each length vocab_size row is the same\n",
    "        top_ks_mask = top_ks_mask.expand(probs_idx.shape[0], -1)\n",
    "        # top_ks is a list of integers. we keep whichever entries in top_ks_mask are greater than their corresponding entries in top_ks\n",
    "        top_ks_mask = top_ks_mask >= top_ks.unsqueeze(dim=1)\n",
    "\n",
    "        # we'll be combining top-p with top-k and using whichever gives us fewer tokens. a very conservative approach\n",
    "        # further trim probs_sort to fit within our top_k requirement\n",
    "        probs_sort = torch.where(top_ks_mask, 0, probs_sort)\n",
    "\n",
    "        # Re-normalization so that total probabilities add up to 1\n",
    "        probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "        # used to rearrange the modified probabilities in probs_sort back to their original order according to probs_idx\n",
    "        probs = torch.gather(probs_sort,\n",
    "                             dim=-1,\n",
    "                             index=torch.argsort(probs_idx, dim=-1))\n",
    "        # samples from the distribution\n",
    "        next_token_ids = torch.multinomial(probs,\n",
    "                                           num_samples=1,\n",
    "                                           replacement=True).squeeze(dim=-1)\n",
    "        \n",
    "        return next_token_ids # returns the predicted tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381acc4a-f442-47e2-953c-7f6efb7b5de5",
   "metadata": {},
   "source": [
    "# Positional Encoding\n",
    "\n",
    "Gemma uses RoPE, which is a popular relative positional encoding scheme. Positional encodings are designed to help the model understand the order of tokens in the text since transformers don't have a built-in ordering to them when reading a sequence. Instead of telling Gemma the precise location of each token (which would be \"absolute\" positional encoding), relative positional encodings only reveal to Gemma the placement of different tokens relative to each other. RoPE is a relative positional encoding scheme that functions within the attention mechanism (rather than, for example, along the embeddings) and uses trigonometry functions to effecitvely \"rotate\" the matrices used in self-attention. RoPE is the standard-to-beat nowadays as it's also find in other notable open-source models like Llama. To see more, check out [the original paper]() or [this more casual guide]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d13e905a-7c60-471f-bf20-8e8bce86bf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Precomputes complex-valued positional encodings (frequencies and phase shifts) to be used in RoPE\n",
    "\n",
    "    Args:\n",
    "    dim (int): The dimensionality of the positional encoding.\n",
    "    end (int): The sequence length or the number of positions to encode.\n",
    "    theta (float, optional): A scaling factor that helps determine the frequencies. Defaults to 10000.0.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: A tensor of shape (end, dim // 2) containing complex-valued positional encodings.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the frequency divisors for even indices (0, 2, ..., dim-2),\n",
    "    # scaled by the dimension and the theta parameter. This creates a geometric\n",
    "    # progression where each frequency is theta^(2i/dim) times smaller than the previous,\n",
    "    # where i is the index in the original dimension space.\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[:(dim // 2)].float() / dim))\n",
    "\n",
    "    # Create a tensor of shape (end,) containing integers from 0 to end-1.\n",
    "    # This tensor represents the position indices for the sequence.\n",
    "    t = torch.arange(end, device=freqs.device)\n",
    "\n",
    "    # Calculate the outer product of position indices and frequency divisors,\n",
    "    # resulting in a matrix where each element (i, j) corresponds to the product\n",
    "    # of the i-th position and the j-th frequency divisor. This effectively scales\n",
    "    # each position index by each frequency divisor, preparing them for conversion\n",
    "    # to phase shifts.\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "\n",
    "    # Convert the scaled position indices into complex numbers with magnitude 1\n",
    "    # and phase equal to the scaled position index. This uses Euler's formula\n",
    "    # e^(i*phi) = cos(phi) + i*sin(phi), where phi is the phase (the scaled position index here).\n",
    "    # The resulting tensor contains complex-valued positional encodings,\n",
    "    # where the real and imaginary parts correspond to the cosine and sine of the\n",
    "    # scaled position indices, respectively.\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "\n",
    "    return freqs_cis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cb0383e-d969-4a79-8722-5c1dcd735727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_emb(x: torch.Tensor, freqs_cis: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Applies rotary positional embeddings to the input tensor `x`, which is either a query or key tensor from the attention mechanism\n",
    "\n",
    "    Parameters:\n",
    "    - x (torch.Tensor): The input tensor to which rotary embeddings are to be applied. \n",
    "    This tensor typically represents the queries and keys in the attention mechanism and is expected to have a shape of (batch_size, num_heads, seq_len, head_dim).\n",
    "    - freqs_cis (torch.Tensor): The precomputed frequency embeddings that are applied to `x`. \n",
    "    This tensor contains complex numbers representing the rotary frequencies, and its shape is expected to match the dimensions of `x` for proper element-wise multiplication.\n",
    "\n",
    "    The function works as follows:\n",
    "    1. The input tensor `x` is first transposed and split into two halves along the last dimension, which correspond to the real and imaginary parts needed for complex multiplication with the frequency embeddings.\n",
    "    2. These parts are then combined into a complex tensor, `x_`, using `torch.view_as_complex`.\n",
    "    3. The rotary embeddings (`freqs_cis`) are applied to `x_` through element-wise multiplication, effectively rotating each element in the complex plane based on its position in the sequence.\n",
    "    4. The resulting complex tensor is then converted back to a real tensor, preserving the original data type of `x`, and is reshaped to match the original dimensions of `x`.\n",
    "    5. Finally, the tensor is re-transposed to its original layout, ready to be used further in the model's computation.\n",
    "\n",
    "    Returns:\n",
    "    - x_out (torch.Tensor): The input tensor `x` after the application of rotary embeddings, ready for use in the attention mechanism.\n",
    "    \"\"\"\n",
    "    x_ = torch.view_as_complex(\n",
    "        torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1),\n",
    "                    dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis).type_as(x)\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    x_out = x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2],\n",
    "                          -1).transpose(1, 2)\n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98920bc2-edc2-4c97-951d-1fdf2799ed6d",
   "metadata": {},
   "source": [
    "# Linear Layers\n",
    "\n",
    "I think later i should be able to delete this and just replace any instance of `Linear(in_features, out_features)` with `nn.linear(in_features, out_features, bias=False)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28b29647-5a96-4d4f-ba48-4bd8e6baebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified linear layer that uses PyTorch's built-in nn.Linear for demonstration and teaching purposes. \n",
    "    This layer does not include a bias term, which the original Gemma does have.\n",
    "\n",
    "    Parameters:\n",
    "    - in_features (int): The size of each input sample (i.e., the number of input features).\n",
    "    - out_features (int): The size of each output sample (i.e., the number of output features).\n",
    "\n",
    "    The forward pass computes the linear transformation of the input data `x` using the layer's weights, without applying any bias.\n",
    "\n",
    "    The forward method:\n",
    "    - x (Tensor): The input tensor to the linear layer with shape (batch_size, *, in_features), where '*' means any number of additional dimensions.\n",
    "\n",
    "    Returns:\n",
    "    - output (Tensor): The output tensor of the linear transformation with shape (batch_size, *, out_features).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super(SimpleLinear, self).__init__()\n",
    "        # Use PyTorch's built-in nn.Linear layer without a bias term\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the built-in nn.Linear layer\n",
    "        output = self.linear(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fbfe3b-f570-46ea-9e44-56be62e957a0",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "same thing here, later i need to replace this with just a raw use of nn.Embedding(num_embeddings, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c140fae4-5b4d-4377-baa9-16097ceda4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified embedding layer that uses PyTorch's built-in nn.Embedding. \n",
    "    This layer maps each index in a range [0, num_embeddings) to a vector of embedding_dim dimensions.\n",
    "\n",
    "    Parameters:\n",
    "    - num_embeddings (int): The size of the dictionary of embeddings, or the maximum integer index that can be looked up.\n",
    "    - embedding_dim (int): The size of each embedding vector; the dimensionality of the embedding space.\n",
    "\n",
    "    This layer does not use quantization to keep the implementation simple and straightforward, which is an option that actual Gemma does inclulde.\n",
    "\n",
    "    The forward method:\n",
    "    - x (Tensor): A LongTensor of arbitrary shape containing the indices to extract embeddings for.\n",
    "\n",
    "    Returns:\n",
    "    - output (Tensor): The embedded tensor, with additional embedding_dim dimension added at the end. If `x` has shape (N, *), `output` will have shape (N, *, embedding_dim).\n",
    "    \"\"\"\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int):\n",
    "        super(SimpleEmbedding, self).__init__()\n",
    "        # Use PyTorch's built-in nn.Embedding layer\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the built-in nn.Embedding layer\n",
    "        output = self.embedding(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756c395f-c4bb-4627-b3d2-a3f6e9a0a365",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "\n",
    "Gemma uses [RMSnorm](). Oddly in Gemma they normalize both the input and the output of each transformer sub-layer, whereas usually you would only normalize in one or the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9740b0d-8c3f-4dc1-8fac-450bb3d63e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the RMS Normalization (Root Mean Square Normalization) layer.\n",
    "    RMSNorm is a variant of layer normalization that normalizes the activations\n",
    "    of the previous layer based on their root mean square value.\n",
    "\n",
    "    Parameters:\n",
    "    - dim (int): The dimension of the input features the normalization is applied to.\n",
    "    - eps (float): A small value added to the denominator for numerical stability. Default is 1e-6.\n",
    "    - add_unit_offset (bool): If True, adds a unit (1) to the learned scaling coefficient, effectively\n",
    "      starting with no scaling. If False, the scaling coefficient starts from zero. Default is True.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        eps: float = 1e-6,\n",
    "        add_unit_offset: bool = True,\n",
    "    ):\n",
    "        super().__init__()  # Initialize the base class (nn.Module)\n",
    "        self.eps = eps  # Small epsilon value for numerical stability\n",
    "        self.add_unit_offset = add_unit_offset  # Flag to determine if a unit should be added to the weight\n",
    "        # Initialize the weight parameter with zeros, which will be learned during training.\n",
    "        # The shape of the weight is [dim], one weight per feature dimension.\n",
    "        self.weight = nn.Parameter(torch.zeros(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        \"\"\"\n",
    "        Private helper function to normalize the input tensor.\n",
    "\n",
    "        Parameters:\n",
    "        - x (Tensor): The input tensor to normalize.\n",
    "\n",
    "        Returns:\n",
    "        - Tensor: The normalized tensor.\n",
    "        \"\"\"\n",
    "        # Calculate the root mean square value for each feature (across the last dimension),\n",
    "        # then use reciprocal square root (rsqrt) for normalization.\n",
    "        # Add self.eps to the denominator for numerical stability.\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the RMSNorm layer.\n",
    "\n",
    "        Parameters:\n",
    "        - x (Tensor): The input tensor to normalize.\n",
    "\n",
    "        Returns:\n",
    "        - output: The normalized and scaled tensor.\n",
    "        \"\"\"\n",
    "        # Normalize the input tensor using the _norm function and ensure the data type matches the input.\n",
    "        x = self._norm(x.float()).type_as(x)\n",
    "        # If add_unit_offset is True, scale the normalized tensor by (1 + self.weight),\n",
    "        # effectively starting with a scaling factor of 1 (no scaling).\n",
    "        # Otherwise, scale by self.weight only.\n",
    "        if self.add_unit_offset:\n",
    "            output = x * (1 + self.weight)\n",
    "        else:\n",
    "            output = x * self.weight\n",
    "        # Return the scaled output tensor.\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ad660e-0c31-427b-af97-a8f21c40ae02",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron\n",
    "\n",
    "The interesting thing here is the GeGLU activation function as opposed to ReLU, GeLU or SwiGLU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3111446c-52ff-4c36-b441-e6724d6ad461",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements a multi-layer perceptron with a GeGLU gating mechanism. The GeGLU\n",
    "    activation combines a standard GeLU activation with a learned gating mechanism, enabling\n",
    "    the network to control the flow of information more dynamically.\n",
    "\n",
    "    Attributes:\n",
    "        gate_proj (nn.Linear): A linear layer that transforms the input tensor to an intermediate\n",
    "                               representation, which is then passed through a GeLU activation for\n",
    "                               gating purposes.\n",
    "        up_proj (nn.Linear): Another linear layer that transforms the input tensor to the same\n",
    "                             intermediate representation but without gating. This representation\n",
    "                             is element-wise multiplied by the gated tensor from `gate_proj`.\n",
    "        down_proj (nn.Linear): A linear layer that transforms the gated and combined tensor back\n",
    "                               to the original dimension of the hidden size, producing the final output.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        intermediate_size: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the GemmaMLP module.\n",
    "\n",
    "        Parameters:\n",
    "            hidden_size (int): The size of the input and output tensors.\n",
    "            intermediate_size (int): The size of the tensor after the initial transformation\n",
    "                                     and before the gating and final projection. This is typically\n",
    "                                     larger than the hidden size to allow for a richer representation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Initialize the parent class (nn.Module)\n",
    "\n",
    "        self.gate_proj = nn.Linear(hidden_size, intermediate_size)\n",
    "        # Linear transformation for the gating mechanism, projecting input to an intermediate size.\n",
    "\n",
    "        self.up_proj = nn.Linear(hidden_size, intermediate_size)\n",
    "        # Linear transformation for the input tensor, also projecting to the intermediate size but\n",
    "        # intended for element-wise multiplication with the gated output.\n",
    "\n",
    "        self.down_proj = nn.Linear(intermediate_size, hidden_size)\n",
    "        # Linear transformation to project the gated and combined tensor back to the original\n",
    "        # hidden size, completing the MLP structure.\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the GemmaMLP module.\n",
    "\n",
    "        Parameters:\n",
    "            x (Tensor): The input tensor to the MLP.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The output tensor after applying the GeGLU gating mechanism and the MLP transformations.\n",
    "        \"\"\"\n",
    "        gate = self.gate_proj(x)\n",
    "        # Applies linear transformation for gating.\n",
    "\n",
    "        gate = F.gelu(gate)\n",
    "        # Applies GeLU activation to the gate, introducing non-linearity and enabling the gating mechanism.\n",
    "\n",
    "        up = self.up_proj(x)\n",
    "        # Applies another linear transformation to the input tensor for subsequent combination with the gate.\n",
    "\n",
    "        fuse = gate * up\n",
    "        # Element-wise multiplication of the gated tensor with the transformed input tensor, modulating\n",
    "        # the input based on the gate's activation.\n",
    "\n",
    "        outputs = self.down_proj(fuse)\n",
    "        # Applies the final linear transformation to project the modulated tensor back to the hidden size.\n",
    "\n",
    "        return outputs\n",
    "        # Returns the final output tensor of the MLP, after gating and modulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e847c83-0fdf-48bd-be37-e225a736531f",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "181d512a-8fa4-4e54-a511-19e07a9fd403",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements Multi-Query Attention which supports a distinct number of attention heads for queries and key-values (KV).\n",
    "    In the case where the same number of queries and key-values are used, this implemenation is equivalent to regular Multi-Head Attention.  \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int, # The dimensionality of input and output tensors.\n",
    "        num_heads: int, # The number of query attention heads\n",
    "        num_kv_heads: int, # The number of key-value heads, which can differ from the query heads\n",
    "        head_dim: int, # The dimensionality of each attention head\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.num_kv_heads = num_kv_heads\n",
    "        \n",
    "        # Ensures that the number of query heads is evenly divisible by the number of KV heads.\n",
    "        assert self.num_heads % self.num_kv_heads == 0\n",
    "        # Determines the number of query heads associated with each KV head.\n",
    "        self.num_queries_per_kv = self.num_heads // self.num_kv_heads\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.head_dim = head_dim\n",
    "\n",
    "        # Calculates the total size for all query projections.\n",
    "        self.q_size = self.num_heads * self.head_dim\n",
    "        # Calculates the total size for all key and value projections.\n",
    "        self.kv_size = self.num_kv_heads * self.head_dim\n",
    "\n",
    "        # Defines the scaling factor for the attention scores.\n",
    "        self.scaling = self.head_dim**-0.5\n",
    "\n",
    "        # Initializes the linear projection layer for queries, keys, and values\n",
    "        self.qkv_proj = Linear(self.hidden_size,\n",
    "                                (self.num_heads + 2 * self.num_kv_heads) * self.head_dim)\n",
    "        # Initializes the output projection layer, mapping the concatenated attention outputs back to the hidden size.\n",
    "        self.o_proj = Linear(self.num_heads * self.head_dim,\n",
    "                                self.hidden_size)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor, # The input tensor to the attention mechanism. shape (batch_size, input_len, hidden_size)\n",
    "        freqs_cis: torch.Tensor, # The frequencies for the rotary position embeddings\n",
    "        kv_write_indices: torch.Tensor, # Indices specifying where to write in the KV cache\n",
    "        kv_cache: Tuple[torch.Tensor, torch.Tensor], # A tuple of tensors holding the cached keys and values\n",
    "        mask: torch.Tensor, # The attention mask tensor, used to exclude future positions from attention calculations\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        # Ensures the input tensor is 3-dimensional (batch size, sequence length, feature dimension).\n",
    "        hidden_states_shape = hidden_states.shape\n",
    "        assert len(hidden_states_shape) == 3\n",
    "\n",
    "        # Extracts batch size and input sequence length from the hidden states tensor.\n",
    "        batch_size, input_len, _ = hidden_states_shape\n",
    "\n",
    "        # Applies the linear projection to the hidden state to retrieve out\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        # Splits the combined QKV tensor into separate tensors for queries (xq), keys (xk), and values (xv) based on their respective sizes.\n",
    "        xq, xk, xv = qkv.split([self.q_size, self.kv_size, self.kv_size],\n",
    "                               dim=-1)\n",
    "        # for readability's sake it would've made more sense to do these separately; this is just more efficient\n",
    "\n",
    "        # Reshapes each of the Q, K, and V tensors to separate the heads and align the dimensions for attention operations.\n",
    "        xq = xq.view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        xk = xk.view(batch_size, -1, self.num_kv_heads, self.head_dim)\n",
    "        xv = xv.view(batch_size, -1, self.num_kv_heads, self.head_dim)\n",
    "\n",
    "        # Applies rotary positional embeddings to queries and keys to incorporate positional information.\n",
    "        xq = apply_rotary_emb(xq, freqs_cis=freqs_cis)\n",
    "        xk = apply_rotary_emb(xk, freqs_cis=freqs_cis)\n",
    "\n",
    "        # Updates the key-value cache with the new keys and values for this forward pass, using provided indices.\n",
    "        # [batch_size, input_len, n_local_kv_heads, head_dim]\n",
    "        k_cache, v_cache = kv_cache\n",
    "        k_cache.index_copy_(1, kv_write_indices, xk)\n",
    "        v_cache.index_copy_(1, kv_write_indices, xv)\n",
    "\n",
    "        key = k_cache\n",
    "        value = v_cache\n",
    "\n",
    "        # If the number of KV heads is different from the number of query heads, adjusts keys and values to match the query heads count.\n",
    "        if self.num_kv_heads != self.num_heads:\n",
    "            # [batch_size, max_seq_len, n_local_heads, head_dim]\n",
    "            key = torch.repeat_interleave(key, \n",
    "                                          self.num_queries_per_kv, \n",
    "                                          dim=2)\n",
    "            value = torch.repeat_interleave(value,\n",
    "                                            self.num_queries_per_kv,\n",
    "                                            dim=2)\n",
    "\n",
    "        # Transposes Q, K, and V tensors to align them for the batch matrix multiplication in attention calculation.\n",
    "        # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        q = xq.transpose(1, 2)\n",
    "        # [batch_size, n_local_heads, max_seq_len, head_dim]\n",
    "        k = key.transpose(1, 2)\n",
    "        v = value.transpose(1, 2)\n",
    "\n",
    "        # Calculates attention scores by performing a batch matrix multiplication between queries and keys, followed by scaling.\n",
    "        # [batch_size, n_local_heads, input_len, max_seq_len]\n",
    "        scores = torch.matmul(q, k.transpose(2, 3)) * self.scaling\n",
    "\n",
    "        # Applies the lower-triangular mask to the attention scores, ensuring future positions are ignored in the attention mechanism.\n",
    "        scores = scores + mask\n",
    "\n",
    "        # Applies softmax to the scores to obtain attention probabilities, \n",
    "        # and ensuring numerical stability by casting to float before softmax and back to original dtype after.\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(q) # i think i can remove the typing stuff since i'm not doing quantization\n",
    "\n",
    "        # Computes the weighted sum of values based on the attention scores to obtain the output of the attention mechanism.\n",
    "        # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.matmul(scores, v)\n",
    "\n",
    "        # Reshapes the attention output to match the expected output dimensions, combining the heads back into the hidden dimension.\n",
    "        # [batch_size, input_len, hidden_dim]\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)\n",
    "\n",
    "        # Applies the final linear projection to the attention output, mapping it back to the hidden size dimension.\n",
    "        output = self.o_proj(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabc735b-8c34-47fb-aea5-6af3a06c616e",
   "metadata": {},
   "source": [
    "# Layer\n",
    "\n",
    "interestingly, here we normalize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbd1fcf1-a1dd-4e6c-8d78-e267398304f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaDecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A decoder layer that integrates the GemmaAttention mechanism and multi-layer perceptron (MLP). It includes\n",
    "    normalization steps both before and after the attention mechanism to stabilize and accelerate training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: GemmaConfig,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initializes the GemmaAttention mechanism with parameters from the config, enabling self-attention within the decoder layer.\n",
    "        self.self_attn = GemmaAttention(\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_heads=config.num_attention_heads,\n",
    "            num_kv_heads=config.num_key_value_heads,\n",
    "            head_dim=config.head_dim,\n",
    "        )\n",
    "        # Initializes the GemmaMLP module, providing a non-linear transformation after the attention mechanism.\n",
    "        self.mlp = GemmaMLP(\n",
    "            hidden_size=config.hidden_size,\n",
    "            intermediate_size=config.intermediate_size,\n",
    "        )\n",
    "        # Applies RMSNorm normalization to the input of the decoder layer for stable training dynamics.\n",
    "        self.input_layernorm = RMSNorm(config.hidden_size,\n",
    "                                       eps=config.rms_norm_eps)\n",
    "        # Applies RMSNorm after the attention mechanism and before the MLP to ensure the output is well-conditioned for further processing.\n",
    "        self.post_attention_layernorm = RMSNorm(config.hidden_size,\n",
    "                                                eps=config.rms_norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor, # The input tensor to the decoder layer. shape (batch_size, input_len, hidden_size)\n",
    "        freqs_cis: torch.Tensor, # The frequencies for the rotary position embeddings, enhancing the model's understanding of sequence order.\n",
    "        kv_write_indices: torch.Tensor, # Indices specifying where to write in the key-value cache, facilitating efficient memory use.\n",
    "        kv_cache: Tuple[torch.Tensor, torch.Tensor], # A cache for keys and values to allow reusing attention computations across decoding steps.\n",
    "        mask: torch.Tensor, # The attention mask tensor, used to exclude future positions from attention calculations\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        # Self Attention Block\n",
    "        # Stores the original input for use as a residual connection, aiding in mitigating the vanishing gradient problem\n",
    "        residual = hidden_states\n",
    "        # Normalizes the input before processing by the attention mechanism.\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "        # Processes the normalized input through the GemmaAttention mechanism\n",
    "        hidden_states = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            freqs_cis=freqs_cis,\n",
    "            kv_write_indices=kv_write_indices,\n",
    "            kv_cache=kv_cache,\n",
    "            mask=mask,\n",
    "        )\n",
    "        # The aforementioned residual connection\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        # MLP Block\n",
    "        # Again, stores the output of the attention block for use as a residual connection before processing by the MLP.\n",
    "        residual = hidden_states\n",
    "        # Normalizes the output of the attention block before passing it to the MLP, ensuring a stable input distribution.\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        # Transforms the normalized attention output through the MLP, introducing additional non-linearity and capacity to the model.\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        # Another residual connection\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7122f7ab-abb0-4c67-9247-a48f705a7bd9",
   "metadata": {},
   "source": [
    "# The Body of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd3b71d6-23da-44a2-94cc-f57102a9aa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaModel(nn.Module):\n",
    "    \"\"\" the class that loops through every layer of the body of Gemma \"\"\"\n",
    "\n",
    "    def __init__(self, config: GemmaConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        # Initialize a sequence of GemmaDecoderLayer instances as specified by the number of hidden layers in the configuration\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(config.num_hidden_layers):\n",
    "            self.layers.append(GemmaDecoderLayer(config))\n",
    "\n",
    "        # Initialize a normalization layer to be applied after the last decoder layer, stabilizing the output\n",
    "        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor, # The input sequence tensor to the model. shape (batch_size, input_len, hidden_size)\n",
    "        freqs_cis: torch.Tensor, # The frequencies for the rotary position embeddings, enhancing the model's understanding of sequence order.\n",
    "        kv_write_indices: torch.Tensor, # Indices specifying where to write in the key-value cache, facilitating efficient memory use.\n",
    "        kv_caches: List[Tuple[torch.Tensor, torch.Tensor]], # A cache for keys and values to allow reusing attention computations across decoding steps\n",
    "        mask: torch.Tensor, # The attention mask tensor, used to exclude future positions from attention calculations\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        \n",
    "        # Iteratively process the input through each GemmaDecoderLayer, passing along necessary parameters for attention and caching.\n",
    "        # The hidden states are updated at each layer, progressively incorporating more complex dependencies and transformations.\n",
    "        for i in range(len(self.layers)):\n",
    "            layer = self.layers[i]\n",
    "            hidden_states = layer(\n",
    "                hidden_states=hidden_states,\n",
    "                freqs_cis=freqs_cis,\n",
    "                kv_write_indices=kv_write_indices,\n",
    "                kv_cache=kv_caches[i],\n",
    "                mask=mask,\n",
    "            )\n",
    "        \n",
    "        # Apply normalization to the output of the final decoder layer, ensuring the model's output is well-conditioned for subsequent use.\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b76ce1-62d9-4594-8775-4c0eb69562a6",
   "metadata": {},
   "source": [
    "# The Model itself, and its generate() and load_weights() functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dea80b7-2d2e-4e0d-9616-6acfee9b4327",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaForCausalLM(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: GemmaConfig,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # the attention heads need to cleanly divide up the hidden_size of the model so that we can split it all apart & combine back together\n",
    "        assert config.hidden_size % config.num_attention_heads == 0\n",
    "\n",
    "        max_seq_len = config.max_position_embeddings\n",
    "        head_dim = config.head_dim\n",
    "        vocab_size = config.vocab_size\n",
    "\n",
    "        #### FIRST PROBLEM HERE. GOTTA FIGURE OUT HOW TO USE MY SIMPLER CHARACTER-WISE TOKENIZER\n",
    "        #self.tokenizer = tokenizer.Tokenizer(config.tokenizer)\n",
    "        \n",
    "        self.embedder = Embedding(vocab_size, config.hidden_size)\n",
    "        self.model = GemmaModel(config)\n",
    "        self.sampler = Sampler(vocab_size)\n",
    "\n",
    "        # Pre-compute rotary embedding table.\n",
    "        rope_theta = getattr(config, 'rope_theta', 10000)\n",
    "        freqs_cis = precompute_freqs_cis(head_dim,\n",
    "                                         max_seq_len * 2,\n",
    "                                         theta=rope_theta)\n",
    "        self.register_buffer('freqs_cis', freqs_cis)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(\n",
    "        self,\n",
    "        input_token_ids: torch.Tensor,\n",
    "        input_positions: torch.Tensor,\n",
    "        kv_write_indices: torch.Tensor,\n",
    "        kv_caches: List[Tuple[torch.Tensor, torch.Tensor]],\n",
    "        mask: torch.Tensor,\n",
    "        output_positions: torch.Tensor,\n",
    "        temperatures: torch.Tensor,\n",
    "        top_ps: torch.Tensor,\n",
    "        top_ks: torch.Tensor,\n",
    "        **kwargs,\n",
    "    ) -> torch.Tensor:\n",
    "        freqs_cis = self.freqs_cis.index_select(0, input_positions)\n",
    "        kv_write_indices = input_positions\n",
    "\n",
    "        # [batch_size, input_len, hidden_size]\n",
    "        hidden_states = self.embedder(input_token_ids)\n",
    "        # Gemma normalizes the embedding by sqrt(hidden_size).\n",
    "        hidden_states = hidden_states * (self.config.hidden_size**0.5)\n",
    "\n",
    "        hidden_states = self.model(\n",
    "            hidden_states=hidden_states,\n",
    "            freqs_cis=freqs_cis,\n",
    "            kv_write_indices=kv_write_indices,\n",
    "            kv_caches=kv_caches,\n",
    "            mask=mask,\n",
    "        )\n",
    "        embedder_weight = self.embedder.weight\n",
    "        if self.config.quant:\n",
    "            embedder_weight = (\n",
    "                embedder_weight * self.embedder.weight_scaler.unsqueeze(-1))\n",
    "        next_tokens = self.sampler(\n",
    "            embedding=embedder_weight,\n",
    "            hidden_states=hidden_states,\n",
    "            output_positions=output_positions,\n",
    "            temperatures=temperatures,\n",
    "            top_ps=top_ps,\n",
    "            top_ks=top_ks,\n",
    "        )\n",
    "        return next_tokens\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        prompts: Union[str, Sequence[str]],\n",
    "        device: Any,\n",
    "        output_len: int = 100,\n",
    "        temperature: float = 0.95,\n",
    "        top_p: float = 1.0,\n",
    "        top_k: int = 100,\n",
    "    ) -> Union[str, Sequence[str]]:\n",
    "        \"\"\"Generates responses for given prompts using Gemma model.\"\"\"\n",
    "        # If a single prompt is provided, treat it as a batch of 1.\n",
    "        is_str_prompt = isinstance(prompts, str)\n",
    "        if is_str_prompt:\n",
    "            prompts = [prompts]\n",
    "\n",
    "        batch_size = len(prompts)\n",
    "        prompt_tokens = [self.tokenizer.encode(prompt) for prompt in prompts]\n",
    "        min_prompt_len = min(len(p) for p in prompt_tokens)\n",
    "        max_prompt_len = max(len(p) for p in prompt_tokens)\n",
    "        max_seq_len = max_prompt_len + output_len\n",
    "        assert max_seq_len <= self.config.max_position_embeddings\n",
    "\n",
    "        # build KV caches\n",
    "        kv_caches = []\n",
    "        for _ in range(self.config.num_hidden_layers):\n",
    "            size = (batch_size, max_seq_len, self.config.num_key_value_heads,\n",
    "                    self.config.head_dim)\n",
    "            dtype = self.config.get_dtype()\n",
    "            k_cache = torch.zeros(size=size, dtype=dtype, device=device)\n",
    "            v_cache = torch.zeros(size=size, dtype=dtype, device=device)\n",
    "            kv_caches.append((k_cache, v_cache))\n",
    "\n",
    "        # prepare inputs\n",
    "        token_ids_tensor = torch.full((batch_size, max_seq_len),\n",
    "                                      self.tokenizer.pad_id, dtype=torch.int64)\n",
    "        input_token_ids_tensor = torch.full((batch_size, min_prompt_len),\n",
    "                                            self.tokenizer.pad_id,\n",
    "                                            dtype=torch.int64)\n",
    "        for i, p in enumerate(prompt_tokens):\n",
    "            token_ids_tensor[i, :len(p)] = torch.tensor(p)\n",
    "            input_token_ids_tensor[i, :min_prompt_len] = torch.tensor(\n",
    "                p[:min_prompt_len])\n",
    "        token_ids_tensor = token_ids_tensor.to(device)\n",
    "        input_token_ids_tensor = input_token_ids_tensor.to(device)\n",
    "        prompt_mask_tensor = token_ids_tensor != self.tokenizer.pad_id\n",
    "        input_positions_tensor = torch.arange(0, min_prompt_len,\n",
    "                                              dtype=torch.int64).to(device)\n",
    "        mask_tensor = torch.full((1, 1, max_seq_len, max_seq_len),\n",
    "                                 -2.3819763e38).to(torch.float)\n",
    "        mask_tensor = torch.triu(mask_tensor, diagonal=1).to(device)\n",
    "        curr_mask_tensor = mask_tensor.index_select(2, input_positions_tensor)\n",
    "        output_positions_tensor = torch.LongTensor([min_prompt_len - 1]).to(\n",
    "            device)\n",
    "        temperatures_tensor = torch.FloatTensor([temperature] * batch_size).to(\n",
    "            device)\n",
    "        top_ps_tensor = torch.FloatTensor([top_p] * batch_size).to(device)\n",
    "        top_ks_tensor = torch.LongTensor([top_k] * batch_size).to(device)\n",
    "        output_index = torch.tensor(min_prompt_len, dtype=torch.int64).to(\n",
    "            device)\n",
    "\n",
    "        # Prefill up to min_prompt_len tokens, then treat other prefill as\n",
    "        # decode and ignore output.\n",
    "        for i in range(max_seq_len - min_prompt_len):\n",
    "            next_token_ids = self(\n",
    "                input_token_ids=input_token_ids_tensor,\n",
    "                input_positions=input_positions_tensor,\n",
    "                kv_write_indices=None,\n",
    "                kv_caches=kv_caches,\n",
    "                mask=curr_mask_tensor,\n",
    "                output_positions=output_positions_tensor,\n",
    "                temperatures=temperatures_tensor,\n",
    "                top_ps=top_ps_tensor,\n",
    "                top_ks=top_ks_tensor,\n",
    "            )\n",
    "\n",
    "            curr_prompt_mask = prompt_mask_tensor.index_select(\n",
    "                1, output_index).squeeze(dim=1)\n",
    "            curr_token_ids = token_ids_tensor.index_select(\n",
    "                1, output_index).squeeze(dim=1)\n",
    "            output_token_ids = torch.where(curr_prompt_mask, curr_token_ids,\n",
    "                                           next_token_ids).unsqueeze(dim=1)\n",
    "            token_ids_tensor.index_copy_(1, output_index, output_token_ids)\n",
    "\n",
    "            input_token_ids_tensor = output_token_ids\n",
    "            input_positions_tensor = output_index.unsqueeze(dim=-1)\n",
    "            curr_mask_tensor = mask_tensor.index_select(2,\n",
    "                                                        input_positions_tensor)\n",
    "            output_positions_tensor = torch.tensor(0, dtype=torch.int64).to(\n",
    "                device)\n",
    "            output_index = output_index + 1\n",
    "\n",
    "        # Detokenization.\n",
    "        token_ids = token_ids_tensor.tolist()\n",
    "        results = []\n",
    "        for i, tokens in enumerate(token_ids):\n",
    "            trimmed_output = tokens[len(prompt_tokens[i]):len(prompt_tokens[i])\n",
    "                                    + output_len]\n",
    "            if self.tokenizer.eos_id in trimmed_output:\n",
    "                eos_index = trimmed_output.index(self.tokenizer.eos_id)\n",
    "                trimmed_output = trimmed_output[:eos_index]\n",
    "            results.append(self.tokenizer.decode(trimmed_output))\n",
    "\n",
    "        # If a string was provided as input, return a string as output.\n",
    "        return results[0] if is_str_prompt else results\n",
    "\n",
    "    def load_weights(self, model_path: str):\n",
    "        self.load_state_dict(\n",
    "            torch.load(model_path, mmap=True)['model_state_dict'],\n",
    "            strict=False,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
